# RESET,ENSURE
[capacity-scheduler]
"yarn.scheduler.capacity.default.minimum-user-limit-percent"|"100"|ENSURE
"yarn.scheduler.capacity.maximum-am-resource-percent"|"0.2"|ENSURE
"yarn.scheduler.capacity.maximum-applications"|"10000"|ENSURE
"yarn.scheduler.capacity.node-locality-delay"|"40"|ENSURE
"yarn.scheduler.capacity.resource-calculator"|"org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator"|ENSURE
"yarn.scheduler.capacity.root.accessible-node-labels"|"*"|ENSURE
"yarn.scheduler.capacity.root.accessible-node-labels.default.capacity"|"-1"|ENSURE
"yarn.scheduler.capacity.root.accessible-node-labels.default.maximum-capacity"|"-1"|ENSURE
"yarn.scheduler.capacity.root.acl_administer_queue"|"*"|ENSURE
"yarn.scheduler.capacity.root.capacity"|"100"|ENSURE
"yarn.scheduler.capacity.root.default-node-label-expression"|" "|ENSURE
"yarn.scheduler.capacity.root.default.acl_administer_jobs"|"*"|ENSURE
"yarn.scheduler.capacity.root.default.acl_submit_applications"|"*"|ENSURE
"yarn.scheduler.capacity.root.default.capacity"|"100"|ENSURE
"yarn.scheduler.capacity.root.default.maximum-capacity"|"100"|ENSURE
"yarn.scheduler.capacity.root.default.state"|"RUNNING"|ENSURE
"yarn.scheduler.capacity.root.default.user-limit-factor"|"1"|ENSURE
"yarn.scheduler.capacity.root.queues"|"default"

[cluster-env]
"hadoop-streaming_tar_destination_folder"|"hdfs:///hdp/apps/{{ hdp_stack_version }}/mapreduce/"|ENSURE
"hadoop-streaming_tar_source"|"/usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar"|ENSURE
"hive_tar_destination_folder"|"hdfs:///hdp/apps/{{ hdp_stack_version }}/hive/"|ENSURE
"hive_tar_source"|"/usr/hdp/current/hive-client/hive.tar.gz"|ENSURE
"ignore_groupsusers_create"|"false"|ENSURE
"kerberos_domain"|"EXAMPLE.COM"|ENSURE
"mapreduce_tar_destination_folder"|"hdfs:///hdp/apps/{{ hdp_stack_version }}/mapreduce/"|ENSURE
"mapreduce_tar_source"|"/usr/hdp/current/hadoop-client/mapreduce.tar.gz"|ENSURE
"pig_tar_destination_folder"|"hdfs:///hdp/apps/{{ hdp_stack_version }}/pig/"|ENSURE
"pig_tar_source"|"/usr/hdp/current/pig-client/pig.tar.gz"|ENSURE
"security_enabled"|"false"|ENSURE
"smokeuser"|"ambari-qa"|ENSURE
"sqoop_tar_destination_folder"|"hdfs:///hdp/apps/{{ hdp_stack_version }}/sqoop/"|ENSURE
"sqoop_tar_source"|"/usr/hdp/current/sqoop-client/sqoop.tar.gz"|ENSURE
"tez_tar_destination_folder"|"hdfs:///hdp/apps/{{ hdp_stack_version }}/tez/"|ENSURE
"tez_tar_source"|"/usr/hdp/current/tez-client/lib/tez.tar.gz"|ENSURE
"user_group"|"hadoop"

[core-site]
"fs.defaultFS"|"hdfs://ip-172-31-14-9.ec2.internal:8020"|ENSURE
"fs.trash.interval"|"360"|ENSURE
"hadoop.http.authentication.simple.anonymous.allowed"|"true"|ENSURE
"hadoop.proxyuser.hcat.groups"|"*"|ENSURE
"hadoop.proxyuser.hcat.hosts"|"*"|ENSURE
"hadoop.proxyuser.hdfs.groups"|"*"|ENSURE
"hadoop.proxyuser.hdfs.hosts"|"ip-172-31-8-195.ec2.internal"|ENSURE
"hadoop.proxyuser.hive.groups"|"users"|ENSURE
"hadoop.proxyuser.hive.hosts"|"ip-172-31-5-57.ec2.internal"|ENSURE
"hadoop.proxyuser.hue.groups"|"*"|ENSURE
"hadoop.proxyuser.hue.hosts"|"*"|ENSURE
"hadoop.proxyuser.oozie.groups"|"*"|ENSURE
"hadoop.proxyuser.oozie.hosts"|"ip-172-31-5-57.ec2.internal"|ENSURE
"hadoop.security.auth_to_local"|"\n RULE:[2:$1@$0]([rn]m@.*)s/.*/yarn/\n RULE:[2:$1@$0](jhs@.*)s/.*/mapred/\n RULE:[2:$1@$0]([nd]n@.*)s/.*/hdfs/\n RULE:[2:$1@$0](hm@.*)s/.*/hbase/\n RULE:[2:$1@$0](rs@.*)s/.*/hbase/\n DEFAULT"|ENSURE
"hadoop.security.authentication"|"simple"|ENSURE
"hadoop.security.authorization"|"false"|ENSURE
"io.compression.codecs"|"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.SnappyCodec"|ENSURE
"io.file.buffer.size"|"131072"|ENSURE
"io.serializations"|"org.apache.hadoop.io.serializer.WritableSerialization"|ENSURE
"ipc.client.connect.max.retries"|"50"|ENSURE
"ipc.client.connection.maxidletime"|"30000"|ENSURE
"ipc.client.idlethreshold"|"8000"|ENSURE
"ipc.server.tcpnodelay"|"true"|ENSURE
"mapreduce.jobtracker.webinterface.trusted"|"false"|ENSURE

[hadoop-env]
"content"|"\n# Set Hadoop-specific environment variables here.\n\n# The only required environment variable is JAVA_HOME. All others are\n# optional. When running a distributed configuration it is best to\n# set JAVA_HOME in this file, so that it is correctly defined on\n# remote nodes.\n\n# The java implementation to use. Required.\nexport JAVA_HOME={{java_home}}\nexport HADOOP_HOME_WARN_SUPPRESS=1\n\n# Hadoop home directory\nexport HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\n# Hadoop Configuration Directory\n\n{# this is different for HDP1 #}\n# Path to jsvc required by secure HDP 2.0 datanode\nexport JSVC_HOME={{jsvc_path}}\n\n\n# The maximum amount of heap to use, in MB. Default is 1000.\nexport HADOOP_HEAPSIZE=\"{{hadoop_heapsize}}\"\n\nexport HADOOP_NAMENODE_INIT_HEAPSIZE=\"-Xms{{namenode_heapsize}}\"\n\n# Extra Java runtime options. Empty by default.\nexport HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true ${HADOOP_OPTS}\"\n\n# Command specific options appended to HADOOP_OPTS when specified\nexport HADOOP_NAMENODE_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{namenode_opt_newsize}} -XX:MaxNewSize={{namenode_opt_maxnewsize}} -XX:PermSize={{namenode_opt_permsize}} -XX:MaxPermSize={{namenode_opt_maxpermsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{namenode_heapsize}} -Xmx{{namenode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_NAMENODE_OPTS}\"\nHADOOP_JOBTRACKER_OPTS=\"-server -XX:ParallelGCThreads=8 -XX:+UseConcMarkSweepGC -XX:ErrorFile={{hdfs_log_dir_prefix}}/$USER/hs_err_pid%p.log -XX:NewSize={{jtnode_opt_newsize}} -XX:MaxNewSize={{jtnode_opt_maxnewsize}} -Xloggc:{{hdfs_log_dir_prefix}}/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xmx{{jtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dmapred.audit.logger=INFO,MRAUDIT -Dhadoop.mapreduce.jobsummary.logger=INFO,JSA ${HADOOP_JOBTRACKER_OPTS}\"\n\nHADOOP_TASKTRACKER_OPTS=\"-server -Xmx{{ttnode_heapsize}} -Dhadoop.security.logger=ERROR,console -Dmapred.audit.logger=ERROR,console ${HADOOP_TASKTRACKER_OPTS}\"\nexport HADOOP_DATANODE_OPTS=\"-server -XX:ParallelGCThreads=4 -XX:+UseConcMarkSweepGC -XX:ErrorFile=/var/log/hadoop/$USER/hs_err_pid%p.log -XX:NewSize=200m -XX:MaxNewSize=200m -XX:PermSize=128m -XX:MaxPermSize=256m -Xloggc:/var/log/hadoop/$USER/gc.log-`date +'%Y%m%d%H%M'` -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xms{{dtnode_heapsize}} -Xmx{{dtnode_heapsize}} -Dhadoop.security.logger=INFO,DRFAS -Dhdfs.audit.logger=INFO,DRFAAUDIT ${HADOOP_DATANODE_OPTS}\"\nHADOOP_BALANCER_OPTS=\"-server -Xmx{{hadoop_heapsize}}m ${HADOOP_BALANCER_OPTS}\"\n\nexport HADOOP_SECONDARYNAMENODE_OPTS=$HADOOP_NAMENODE_OPTS\n\n# The following applies to multiple commands (fs, dfs, fsck, distcp etc)\nexport HADOOP_CLIENT_OPTS=\"-Xmx${HADOOP_HEAPSIZE}m -XX:MaxPermSize=512m $HADOOP_CLIENT_OPTS\"\n\n# On secure datanodes, user to run the datanode as after dropping privileges\nexport HADOOP_SECURE_DN_USER=${HADOOP_SECURE_DN_USER:-{{hadoop_secure_dn_user}}}\n\n# Extra ssh options. Empty by default.\nexport HADOOP_SSH_OPTS=\"-o ConnectTimeout=5 -o SendEnv=HADOOP_CONF_DIR\"\n\n# Where log files are stored. $HADOOP_HOME/logs by default.\nexport HADOOP_LOG_DIR={{hdfs_log_dir_prefix}}/$USER\n\n# History server logs\nexport HADOOP_MAPRED_LOG_DIR={{mapred_log_dir_prefix}}/$USER\n\n# Where log files are stored in the secure data environment.\nexport HADOOP_SECURE_DN_LOG_DIR={{hdfs_log_dir_prefix}}/$HADOOP_SECURE_DN_USER\n\n# File naming remote slave hosts. $HADOOP_HOME/conf/slaves by default.\n# export HADOOP_SLAVES=${HADOOP_HOME}/conf/slaves\n\n# host:path where hadoop code should be rsync'd from. Unset by default.\n# export HADOOP_MASTER=master:/home/$USER/src/hadoop\n\n# Seconds to sleep between slave commands. Unset by default. This\n# can be useful in large clusters, where, e.g., slave rsyncs can\n# otherwise arrive faster than the master can service them.\n# export HADOOP_SLAVE_SLEEP=0.1\n\n# The directory where pid files are stored. /tmp by default.\nexport HADOOP_PID_DIR={{hadoop_pid_dir_prefix}}/$USER\nexport HADOOP_SECURE_DN_PID_DIR={{hadoop_pid_dir_prefix}}/$HADOOP_SECURE_DN_USER\n\n# History server pid\nexport HADOOP_MAPRED_PID_DIR={{mapred_pid_dir_prefix}}/$USER\n\nYARN_RESOURCEMANAGER_OPTS=\"-Dyarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY\"\n\n# A string representing this instance of hadoop. $USER by default.\nexport HADOOP_IDENT_STRING=$USER\n\n# The scheduling priority for daemon processes. See 'man nice'.\n\n# export HADOOP_NICENESS=10\n\n# Use libraries from standard classpath\nJAVA_JDBC_LIBS=\"\"\n#Add libraries required by mysql connector\nfor jarFile in `ls /usr/share/java/mysql-connector-java-5.1.17.jar /usr/share/java/mysql-connector-java-5.1.34-bin.jar /usr/share/java/mysql-connector-java.jar 2>/dev/null`\ndo\n JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\ndone\n# Add libraries required by oracle connector\nfor jarFile in `ls /usr/share/java/*ojdbc* 2>/dev/null`\ndo\n JAVA_JDBC_LIBS=${JAVA_JDBC_LIBS}:$jarFile\ndone\n# Add libraries required by nodemanager\nMAPREDUCE_LIBS={{mapreduce_libs_path}}\nexport HADOOP_CLASSPATH=${HADOOP_CLASSPATH}${JAVA_JDBC_LIBS}:${MAPREDUCE_LIBS}\n\n# added to the HADOOP_CLASSPATH\nif [ -d \"/usr/hdp/current/tez-client\" ]; then\n if [ -d \"/etc/tez/conf/\" ]; then\n # When using versioned RPMs, the tez-client will be a symlink to the current folder of tez in HDP.\n export HADOOP_CLASSPATH=${HADOOP_CLASSPATH}:/usr/hdp/current/tez-client/*:/usr/hdp/current/tez-client/lib/*:/etc/tez/conf/\n fi\nfi\n\n# Setting path to hdfs command line\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\n\n# Mostly required for hadoop 2.0\nexport JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}\n\nexport HADOOP_OPTS=\"-Dhdp.version=$HDP_VERSION $HADOOP_OPTS\""|ENSURE
"dfs.datanode.data.dir.mount.file"|"/etc/hadoop/conf/dfs_data_dir_mount.hist"|ENSURE
"dtnode_heapsize"|"1024m"|ENSURE
"hadoop_heapsize"|"1024"|ENSURE
"hadoop_pid_dir_prefix"|"/var/run/hadoop"|ENSURE
"hadoop_root_logger"|"INFO,RFA"|ENSURE
"hdfs_log_dir_prefix"|"/hadoop/logs"|ENSURE
"hdfs_user"|"hdfs"|ENSURE
"namenode_heapsize"|"4096m"|ENSURE
"namenode_opt_maxnewsize"|"200m"|ENSURE
"namenode_opt_maxpermsize"|"256m"|ENSURE
"namenode_opt_newsize"|"200m"|ENSURE
"namenode_opt_permsize"|"128m"|ENSURE
"proxyuser_group"|"users"|ENSURE

[hadoop-policy]
"security.admin.operations.protocol.acl"|"hadoop"|ENSURE
"security.client.datanode.protocol.acl"|"*"|ENSURE
"security.client.protocol.acl"|"*"|ENSURE
"security.datanode.protocol.acl"|"*"|ENSURE
"security.inter.datanode.protocol.acl"|"*"|ENSURE
"security.inter.tracker.protocol.acl"|"*"|ENSURE
"security.job.client.protocol.acl"|"*"|ENSURE
"security.job.task.protocol.acl"|"*"|ENSURE
"security.namenode.protocol.acl"|"*"|ENSURE
"security.refresh.policy.protocol.acl"|"hadoop"|ENSURE
"security.refresh.usertogroups.mappings.protocol.acl"|"hadoop"|ENSURE

[hbase-env]

[hbase-site]

[hcat-env]
"content"|"\n # Licensed to the Apache Software Foundation (ASF) under one\n # or more contributor license agreements. See the NOTICE file\n # distributed with this work for additional information\n # regarding copyright ownership. The ASF licenses this file\n # to you under the Apache License, Version 2.0 (the\n # \"License\"); you may not use this file except in compliance\n # with the License. You may obtain a copy of the License at\n #\n # http://www.apache.org/licenses/LICENSE-2.0\n #\n # Unless required by applicable law or agreed to in writing, software\n # distributed under the License is distributed on an \"AS IS\" BASIS,\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n\n JAVA_HOME={{java64_home}}\n HCAT_PID_DIR={{hcat_pid_dir}}/\n HCAT_LOG_DIR={{hcat_log_dir}}/\n HCAT_CONF_DIR={{hcat_conf_dir}}\n HADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n #DBROOT is the path where the connector jars are downloaded\n DBROOT={{hcat_dbroot}}\n USER={{hcat_user}}\n METASTORE_PORT={{hive_metastore_port}}"

[hdfs-log4j]
"content"|"\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements. See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership. The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License. You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied. See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n\n\n# Define some default values that can be overridden by system properties\n# To change daemon root logger use hadoop_root_logger in hadoop-env\nhadoop.root.logger=INFO,console\nhadoop.log.dir=.\nhadoop.log.file=hadoop.log\n\n\n# Define the root logger to the system property \"hadoop.root.logger\".\nlog4j.rootLogger=${hadoop.root.logger}, EventCounter\n\n# Logging Threshold\nlog4j.threshhold=ALL\n\n#\n# Daily Rolling File Appender\n#\n\nlog4j.appender.DRFA=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\n# Rollver at midnight\nlog4j.appender.DRFA.DatePattern=.yyyy-MM-dd\n\n# 30-day backup\n#log4j.appender.DRFA.MaxBackupIndex=30\nlog4j.appender.DRFA.layout=org.apache.log4j.PatternLayout\n\n# Pattern format: Date LogLevel LoggerName LogMessage\nlog4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n# Debugging Pattern format\n#log4j.appender.DRFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\n\n\n#\n# console\n# Add \"console\" to rootlogger above if you want to use this\n#\n\nlog4j.appender.console=org.apache.log4j.ConsoleAppender\nlog4j.appender.console.target=System.err\nlog4j.appender.console.layout=org.apache.log4j.PatternLayout\nlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\n\n#\n# TaskLog Appender\n#\n\n#Default values\nhadoop.tasklog.taskid=null\nhadoop.tasklog.iscleanup=false\nhadoop.tasklog.noKeepSplits=4\nhadoop.tasklog.totalLogFileSize=100\nhadoop.tasklog.purgeLogSplits=true\nhadoop.tasklog.logsRetainHours=12\n\nlog4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender\nlog4j.appender.TLA.taskId=${hadoop.tasklog.taskid}\nlog4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}\nlog4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}\n\nlog4j.appender.TLA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\n\n#\n#Security audit appender\n#\nhadoop.security.logger=INFO,console\nhadoop.security.log.maxfilesize=256MB\nhadoop.security.log.maxbackupindex=20\nlog4j.category.SecurityLogger=${hadoop.security.logger}\nhadoop.security.log.file=SecurityAuth.audit\nlog4j.appender.DRFAS=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.DRFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.DRFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\nlog4j.appender.DRFAS.DatePattern=.yyyy-MM-dd\n\nlog4j.appender.RFAS=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFAS.File=${hadoop.log.dir}/${hadoop.security.log.file}\nlog4j.appender.RFAS.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFAS.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n\nlog4j.appender.RFAS.MaxFileSize=${hadoop.security.log.maxfilesize}\nlog4j.appender.RFAS.MaxBackupIndex=${hadoop.security.log.maxbackupindex}\n\n#\n# hdfs audit logging\n#\nhdfs.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=${hdfs.audit.logger}\nlog4j.additivity.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit=false\nlog4j.appender.DRFAAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.DRFAAUDIT.File=${hadoop.log.dir}/hdfs-audit.log\nlog4j.appender.DRFAAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.DRFAAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.DRFAAUDIT.DatePattern=.yyyy-MM-dd\n\n#\n# mapred audit logging\n#\nmapred.audit.logger=INFO,console\nlog4j.logger.org.apache.hadoop.mapred.AuditLogger=${mapred.audit.logger}\nlog4j.additivity.org.apache.hadoop.mapred.AuditLogger=false\nlog4j.appender.MRAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.MRAUDIT.File=${hadoop.log.dir}/mapred-audit.log\nlog4j.appender.MRAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.MRAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.MRAUDIT.DatePattern=.yyyy-MM-dd\n\n#\n# Rolling File Appender\n#\n\nlog4j.appender.RFA=org.apache.log4j.RollingFileAppender\nlog4j.appender.RFA.File=${hadoop.log.dir}/${hadoop.log.file}\n\n# Logfile size and and 30-day backups\nlog4j.appender.RFA.MaxFileSize=256MB\nlog4j.appender.RFA.MaxBackupIndex=10\n\nlog4j.appender.RFA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} - %m%n\nlog4j.appender.RFA.layout.ConversionPattern=%d{ISO8601} %-5p %c{2} (%F:%M(%L)) - %m%n\n\n\n# Custom Logging levels\n\nhadoop.metrics.log.level=INFO\n#log4j.logger.org.apache.hadoop.mapred.JobTracker=DEBUG\n#log4j.logger.org.apache.hadoop.mapred.TaskTracker=DEBUG\n#log4j.logger.org.apache.hadoop.fs.FSNamesystem=DEBUG\nlog4j.logger.org.apache.hadoop.metrics2=${hadoop.metrics.log.level}\n\n# Jets3t library\nlog4j.logger.org.jets3t.service.impl.rest.httpclient.RestS3Service=ERROR\n\n#\n# Null Appender\n# Trap security logger on the hadoop client side\n#\nlog4j.appender.NullAppender=org.apache.log4j.varia.NullAppender\n\n#\n# Event Counter Appender\n# Sends counts of logging messages at different severity levels to Hadoop Metrics.\n#\nlog4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter\n\n# Removes \"deprecated\" messages\nlog4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN\n\n#\n# HDFS block state change log from block manager\n#\n# Uncomment the following to suppress normal block state change\n# messages from BlockManager in NameNode.\n#log4j.logger.BlockStateChange=WARN"

[hdfs-site]
"dfs.block.access.token.enable"|"true"|ENSURE
"dfs.blockreport.initialDelay"|"120"|ENSURE
"dfs.blocksize"|"134217728"|ENSURE
"dfs.client.read.shortcircuit"|"true"|ENSURE
"dfs.client.read.shortcircuit.streams.cache.size"|"4096"|ENSURE
"dfs.cluster.administrators"|" hdfs"|ENSURE
"dfs.datanode.address"|"0.0.0.0:50010"|ENSURE
"dfs.datanode.balance.bandwidthPerSec"|"6250000"|ENSURE
"dfs.datanode.data.dir"|"/tmp/data"|ENSURE
"dfs.datanode.data.dir.perm"|"750"|ENSURE
"dfs.datanode.du.reserved"|"1073741824"|ENSURE
"dfs.datanode.failed.volumes.tolerated"|"0"|ENSURE
"dfs.datanode.http.address"|"0.0.0.0:50075"|ENSURE
"dfs.datanode.https.address"|"0.0.0.0:50475"|ENSURE
"dfs.datanode.ipc.address"|"0.0.0.0:8010"|ENSURE
"dfs.datanode.max.transfer.threads"|"4096"|ENSURE
"dfs.domain.socket.path"|"/var/lib/hadoop-hdfs/dn_socket"|ENSURE
"dfs.heartbeat.interval"|"3"|ENSURE
"dfs.hosts.exclude"|"/etc/hadoop/conf/dfs.exclude"|ENSURE
"dfs.http.policy"|"HTTP_ONLY"|ENSURE
"dfs.https.port"|"50470"|ENSURE
"dfs.journalnode.edits.dir"|"/tmp/hdfs/journalnode"|ENSURE
"dfs.journalnode.http-address"|"0.0.0.0:8480"|ENSURE
"dfs.namenode.accesstime.precision"|"3600000"|ENSURE
"dfs.namenode.avoid.read.stale.datanode"|"true"|ENSURE
"dfs.namenode.avoid.write.stale.datanode"|"true"|ENSURE
"dfs.namenode.checkpoint.dir"|"/tmp/hdfs/namesecondary"|ENSURE_ALERT
"dfs.namenode.checkpoint.edits.dir"|"${dfs.namenode.checkpoint.dir}"|ENSURE
"dfs.namenode.checkpoint.period"|"21600"|ENSURE
"dfs.namenode.checkpoint.txns"|"1000000"|ENSURE
"dfs.namenode.handler.count"|"40"|ENSURE
"dfs.namenode.http-address"|"localhost:50070"|ENSURE_ALERT
"dfs.namenode.https-address"|"localhost:50470"|ENSURE_ALERT
"dfs.namenode.name.dir"|"/tmp/hdfs/namenode"|ENSURE_ALERT
"dfs.namenode.name.dir.restore"|"true"|ENSURE
"dfs.namenode.safemode.threshold-pct"|"1.0f"|ENSURE
"dfs.namenode.secondary.http-address"|"localhost:50090"|ENSURE_ALERT
"dfs.namenode.stale.datanode.interval"|"30000"|ENSURE
"dfs.namenode.startup.delay.block.deletion.sec"|"3600"|ENSURE
"dfs.namenode.write.stale.datanode.ratio"|"1.0f"|ENSURE
#"dfs.nfs.exports.allowed.hosts"|"* rw"|ENSURE
#"dfs.nfs3.dump.dir"|"/data/hdfs-nfs-tmp/cache"|ENSURE
"dfs.permissions.enabled"|"true"|ENSURE
"dfs.permissions.superusergroup"|"hdfs"|ENSURE
"dfs.replication"|"3"|ENSURE
"dfs.replication.max"|"50"|ENSURE
"dfs.support.append"|"true"|ENSURE
"dfs.webhdfs.enabled"|"true"|ENSURE
"fs.permissions.umask-mode"|"022"|ENSURE

[hive-env]
"content"|"\n if [ \"$SERVICE\" = \"cli\" ]; then\n if [ -z \"$DEBUG\" ]; then\n export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -Xms10m -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:+UseParNewGC -XX:-UseGCOverheadLimit\"\n else\n export HADOOP_OPTS=\"$HADOOP_OPTS -XX:NewRatio=12 -Xms10m -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:-UseGCOverheadLimit\"\n fi\n fi\n\n# The heap size of the jvm stared by hive shell script can be controlled via:\n\nexport HADOOP_HEAPSIZE=\"{{hive_heapsize}}\"\nexport HADOOP_CLIENT_OPTS=\"-Xmx${HADOOP_HEAPSIZE}m $HADOOP_CLIENT_OPTS\"\n\n# Larger heap size may be required when running queries over large number of files or partitions.\n# By default hive shell scripts use a heap size of 256 (MB). Larger heap size would also be\n# appropriate for hive server (hwi etc).\n\n\n# Set HADOOP_HOME to point to a specific hadoop install directory\nHADOOP_HOME=${HADOOP_HOME:-{{hadoop_home}}}\n\n# Hive Configuration Directory can be controlled by:\nexport HIVE_CONF_DIR={{hive_config_dir}}\n\n# Folder containing extra libraries required for hive compilation/execution can be controlled by:\nif [ \"${HIVE_AUX_JARS_PATH}\" != \"\" ]; then\n export HIVE_AUX_JARS_PATH=${HIVE_AUX_JARS_PATH}\nelif [ -d \"/usr/hdp/current/hive-webhcat/share/hcatalog\" ]; then\n export HIVE_AUX_JARS_PATH=/usr/hdp/current/hive-webhcat/share/hcatalog\nfi\n\nexport METASTORE_PORT={{hive_metastore_port}}"|RESET
"hcat_log_dir"|"/var/log/webhcat"|ENSURE
"hcat_pid_dir"|"/var/run/webhcat"|ENSURE
"hcat_user"|"hcat"|ENSURE
#"hive_database"|"Existing MySQL Database"|ENSURE
#"hive_database_name"|"hive"|ENSURE
#"hive_database_type"|"mysql"|ENSURE
#"hive_existing_mysql_database"|"MySQL"|ENSURE
#"hive_hostname"|"localhost"|ENSURE
"hive_log_dir"|"/var/logs/hive"|ENSURE
"hive_metastore_port"|"9083"|ENSURE
"hive_pid_dir"|"/var/run/hive"|ENSURE
"hive_user"|"hive"|ENSURE
"webhcat_user"|"hcat"

[hive-site]
"ambari.hive.db.schema.name"|"hive"|ENSURE
"datanucleus.cache.level2.type"|"none"|ENSURE
"hive.auto.convert.join"|"true"|ENSURE
"hive.auto.convert.join.noconditionaltask"|"true"|ENSURE
"hive.auto.convert.join.noconditionaltask.size"|"1073741824"|ENSURE
"hive.auto.convert.sortmerge.join"|"true"|ENSURE
"hive.auto.convert.sortmerge.join.noconditionaltask"|"true"|ENSURE
"hive.auto.convert.sortmerge.join.to.mapjoin"|"false"|ENSURE
"hive.cbo.enable"|"true"|ENSURE
"hive.cli.print.header"|"false"|ENSURE
"hive.cluster.delegation.token.store.class"|"org.apache.hadoop.hive.thrift.ZooKeeperTokenStore"|ENSURE
"hive.cluster.delegation.token.store.zookeeper.connectString"|"localhost1:2181,localhost2:2181,localhost3:2181"|ENSURE_ALERT
"hive.cluster.delegation.token.store.zookeeper.znode"|"/hive/cluster/delegation"|ENSURE
"hive.compactor.abortedtxn.threshold"|"1000"|ENSURE
"hive.compactor.check.interval"|"300L"|ENSURE
"hive.compactor.delta.num.threshold"|"10"|ENSURE
"hive.compactor.delta.pct.threshold"|"0.1f"|ENSURE
"hive.compactor.initiator.on"|"false"|ENSURE
"hive.compactor.worker.threads"|"0"|ENSURE
"hive.compactor.worker.timeout"|"86400L"|ENSURE
"hive.compute.query.using.stats"|"true"|ENSURE
"hive.conf.restricted.list"|"hive.security.authenticator.manager,hive.security.authorization.manager,hive.users.in.admin.role"|ENSURE
"hive.convert.join.bucket.mapjoin.tez"|"false"|ENSURE
"hive.enforce.bucketing"|"true"|ENSURE
"hive.enforce.sorting"|"true"|ENSURE
"hive.enforce.sortmergebucketmapjoin"|"true"|ENSURE
"hive.exec.compress.intermediate"|"false"|ENSURE
"hive.exec.compress.output"|"false"|ENSURE
"hive.exec.dynamic.partition"|"true"|ENSURE
"hive.exec.dynamic.partition.mode"|"nonstrict"|ENSURE
"hive.exec.failure.hooks"|"org.apache.hadoop.hive.ql.hooks.ATSHook"|ENSURE
"hive.exec.max.created.files"|"100000"|ENSURE
"hive.exec.max.dynamic.partitions"|"5000"|ENSURE
"hive.exec.max.dynamic.partitions.pernode"|"2000"|ENSURE
"hive.exec.orc.compression.strategy"|"SPEED"|ENSURE
"hive.exec.orc.default.compress"|"ZLIB"|ENSURE
"hive.exec.orc.default.stripe.size"|"67108864"|ENSURE
"hive.exec.parallel"|"false"|ENSURE
"hive.exec.parallel.thread.number"|"8"|ENSURE
"hive.exec.post.hooks"|"org.apache.hadoop.hive.ql.hooks.ATSHook"|ENSURE
"hive.exec.pre.hooks"|"org.apache.hadoop.hive.ql.hooks.ATSHook"|ENSURE
"hive.exec.reducers.bytes.per.reducer"|"67108864"|ENSURE
"hive.exec.reducers.max"|"1009"|ENSURE
"hive.exec.scratchdir"|"/tmp/hive"|ENSURE
"hive.exec.submit.local.task.via.child"|"true"|ENSURE
"hive.exec.submitviachild"|"false"|ENSURE
"hive.execution.engine"|"tez"|ENSURE
"hive.fetch.task.aggr"|"false"|ENSURE
"hive.fetch.task.conversion"|"more"|ENSURE
"hive.fetch.task.conversion.threshold"|"1073741824"|ENSURE
"hive.heapsize"|"1024"|ENSURE
"hive.limit.optimize.enable"|"true"|ENSURE
"hive.limit.pushdown.memory.usage"|"0.04"|ENSURE
"hive.map.aggr"|"true"|ENSURE
"hive.map.aggr.hash.force.flush.memory.threshold"|"0.9"|ENSURE
"hive.map.aggr.hash.min.reduction"|"0.5"|ENSURE
"hive.map.aggr.hash.percentmemory"|"0.5"|ENSURE
"hive.mapjoin.bucket.cache.size"|"10000"|ENSURE
"hive.mapjoin.optimized.hashtable"|"true"|ENSURE
"hive.mapred.reduce.tasks.speculative.execution"|"false"|ENSURE
"hive.merge.mapfiles"|"true"|ENSURE
"hive.merge.mapredfiles"|"false"|ENSURE
"hive.merge.orcfile.stripe.level"|"true"|ENSURE
"hive.merge.rcfile.block.level"|"true"|ENSURE
"hive.merge.size.per.task"|"256000000"|ENSURE
"hive.merge.smallfiles.avgsize"|"16000000"|ENSURE
"hive.merge.tezfiles"|"false"|ENSURE
"hive.metastore.authorization.storage.checks"|"false"|ENSURE
"hive.metastore.cache.pinobjtypes"|"Table,Database,Type,FieldSchema,Order"|ENSURE
"hive.metastore.client.connect.retry.delay"|"5s"|ENSURE
"hive.metastore.client.socket.timeout"|"1800s"|ENSURE
"hive.metastore.connect.retries"|"24"|ENSURE
"hive.metastore.execute.setugi"|"true"|ENSURE
"hive.metastore.failure.retries"|"24"|ENSURE
"hive.metastore.kerberos.keytab.file"|"/etc/security/keytabs/hive.service.keytab"|ENSURE
"hive.metastore.kerberos.principal"|"hive/_HOST@EXAMPLE.COM"|ENSURE
"hive.metastore.pre.event.listeners"|"org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener"|ENSURE
"hive.metastore.sasl.enabled"|"false"|ENSURE
"hive.metastore.server.max.threads"|"100000"|ENSURE
"hive.metastore.uris"|"thrift://localhost:9083"|ENSURE_ALERT
"hive.metastore.warehouse.dir"|"/apps/hive/warehouse"|ENSURE
"hive.optimize.bucketmapjoin"|"true"|ENSURE
"hive.optimize.bucketmapjoin.sortedmerge"|"false"|ENSURE
"hive.optimize.constant.propagation"|"true"|ENSURE
"hive.optimize.index.filter"|"true"|ENSURE
"hive.optimize.mapjoin.mapreduce"|"true"|ENSURE
"hive.optimize.metadataonly"|"true"|ENSURE
"hive.optimize.null.scan"|"true"|ENSURE
"hive.optimize.reducededuplication"|"true"|ENSURE
"hive.optimize.reducededuplication.min.reducer"|"4"|ENSURE
"hive.optimize.sort.dynamic.partition"|"false"|ENSURE
"hive.orc.compute.splits.num.threads"|"10"|ENSURE
"hive.orc.splits.include.file.footer"|"false"|ENSURE
"hive.prewarm.enabled"|"false"|ENSURE
"hive.prewarm.numcontainers"|"10"|ENSURE
"hive.security.authenticator.manager"|"org.apache.hadoop.hive.ql.security.ProxyUserAuthenticator"|ENSURE
"hive.security.authorization.enabled"|"false"|ENSURE
"hive.security.authorization.manager"|"org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory"|ENSURE
"hive.security.metastore.authenticator.manager"|"org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator"|ENSURE
"hive.security.metastore.authorization.auth.reads"|"true"|ENSURE
"hive.security.metastore.authorization.manager"|"org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider,org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly"|ENSURE
"hive.server2.allow.user.substitution"|"true"|ENSURE
"hive.server2.authentication"|"NONE"|ENSURE
"hive.server2.authentication.spnego.keytab"|"HTTP/_HOST@EXAMPLE.COM"|ENSURE
"hive.server2.authentication.spnego.principal"|"/etc/security/keytabs/spnego.service.keytab"|ENSURE
"hive.server2.enable.doAs"|"false"|ENSURE
"hive.server2.enable.impersonation"|"true"|ENSURE
"hive.server2.logging.operation.enabled"|"true"|ENSURE
"hive.server2.logging.operation.log.location"|"${system:java.io.tmpdir}/${system:user.name}/operation_logs"|ENSURE
"hive.server2.support.dynamic.service.discovery"|"true"|ENSURE
"hive.server2.table.type.mapping"|"CLASSIC"|ENSURE
"hive.server2.tez.default.queues"|"default"|ENSURE
"hive.server2.tez.initialize.default.sessions"|"false"|ENSURE
"hive.server2.tez.sessions.per.default.queue"|"1"|ENSURE
"hive.server2.thrift.http.path"|"cliservice"|ENSURE
"hive.server2.thrift.http.port"|"10001"|ENSURE
"hive.server2.thrift.max.worker.threads"|"500"|ENSURE
"hive.server2.thrift.port"|"10000"|ENSURE
"hive.server2.thrift.sasl.qop"|"auth"|ENSURE
"hive.server2.transport.mode"|"binary"|ENSURE
"hive.server2.use.SSL"|"false"|ENSURE
"hive.server2.zookeeper.namespace"|"hiveserver2"|ENSURE
"hive.smbjoin.cache.rows"|"10000"|ENSURE
"hive.stats.autogather"|"true"|ENSURE
"hive.stats.dbclass"|"fs"|ENSURE
"hive.stats.fetch.column.stats"|"false"|ENSURE
"hive.stats.fetch.partition.stats"|"true"|ENSURE
"hive.support.concurrency"|"false"|ENSURE
"hive.tez.auto.reducer.parallelism"|"false"|ENSURE
"hive.tez.container.size"|"3072"|ENSURE
"hive.tez.cpu.vcores"|"-1"|ENSURE
"hive.tez.dynamic.partition.pruning"|"true"|ENSURE
"hive.tez.dynamic.partition.pruning.max.data.size"|"104857600"|ENSURE
"hive.tez.dynamic.partition.pruning.max.event.size"|"1048576"|ENSURE
"hive.tez.input.format"|"org.apache.hadoop.hive.ql.io.HiveInputFormat"|ENSURE
"hive.tez.java.opts"|"-server -Xmx2458m -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+UseParallelGC -XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps"|ENSURE
"hive.tez.log.level"|"INFO"|ENSURE
"hive.tez.max.partition.factor"|"2.0"|ENSURE
"hive.tez.min.partition.factor"|"0.25"|ENSURE
"hive.tez.smb.number.waves"|"0.5"|ENSURE
"hive.txn.manager"|"org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager"|ENSURE
"hive.txn.max.open.batch"|"1000"|ENSURE
"hive.txn.timeout"|"300"|ENSURE
"hive.user.install.directory"|"/user/"|ENSURE
"hive.vectorized.execution.enabled"|"true"|ENSURE
"hive.vectorized.execution.reduce.enabled"|"false"|ENSURE
"hive.vectorized.groupby.checkinterval"|"4096"|ENSURE
"hive.vectorized.groupby.flush.percent"|"0.1"|ENSURE
"hive.vectorized.groupby.maxentries"|"100000"|ENSURE
"hive.zookeeper.client.port"|"2181"|ENSURE
"hive.zookeeper.namespace"|"hive_zookeeper_namespace"|ENSURE
"hive.zookeeper.quorum"|"localhost1:2181,localhost2:2181,localhost3:2181"|ENSURE_ALERT

[mapred-site]
"mapreduce.admin.map.child.java.opts"|"-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}"|ENSURE
"mapreduce.admin.reduce.child.java.opts"|"-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}"|ENSURE
"mapreduce.admin.user.env"|"LD_LIBRARY_PATH=/usr/hdp/${hdp.version}/hadoop/lib/native:/usr/hdp/${hdp.version}/hadoop/lib/native/Linux-amd64-64"|ENSURE
"mapreduce.am.max-attempts"|"2"|ENSURE
"mapreduce.application.classpath"|"$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure"|ENSURE
"mapreduce.application.framework.path"|"/hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework"|ENSURE
"mapreduce.cluster.administrators"|" hadoop"|ENSURE
"mapreduce.framework.name"|"yarn"|ENSURE
"mapreduce.job.emit-timeline-data"|"false"|ENSURE
"mapreduce.job.reduce.slowstart.completedmaps"|"0.05"|ENSURE
"mapreduce.jobhistory.address"|"ip-172-31-5-57.ec2.internal:10020"|ENSURE
"mapreduce.jobhistory.bind-host"|"0.0.0.0"|ENSURE
"mapreduce.jobhistory.done-dir"|"/mr-history/done"|ENSURE
"mapreduce.jobhistory.intermediate-done-dir"|"/mr-history/tmp"|ENSURE
"mapreduce.jobhistory.webapp.address"|"ip-172-31-5-57.ec2.internal:19888"|ENSURE
"mapreduce.map.java.opts"|"-Xmx2458m"|ENSURE
"mapreduce.map.log.level"|"INFO"|ENSURE
"mapreduce.map.memory.mb"|"3072"|ENSURE
"mapreduce.map.output.compress"|"false"|ENSURE
"mapreduce.map.sort.spill.percent"|"0.7"|ENSURE
"mapreduce.map.speculative"|"false"|ENSURE
"mapreduce.output.fileoutputformat.compress"|"false"|ENSURE
"mapreduce.output.fileoutputformat.compress.type"|"BLOCK"|ENSURE
"mapreduce.reduce.input.buffer.percent"|"0.0"|ENSURE
"mapreduce.reduce.java.opts"|"-Xmx2458m"|ENSURE
"mapreduce.reduce.log.level"|"INFO"|ENSURE
"mapreduce.reduce.memory.mb"|"3072"|ENSURE
"mapreduce.reduce.shuffle.fetch.retry.enabled"|"1"|ENSURE
"mapreduce.reduce.shuffle.fetch.retry.interval-ms"|"1000"|ENSURE
"mapreduce.reduce.shuffle.fetch.retry.timeout-ms"|"30000"|ENSURE
"mapreduce.reduce.shuffle.input.buffer.percent"|"0.7"|ENSURE
"mapreduce.reduce.shuffle.merge.percent"|"0.66"|ENSURE
"mapreduce.reduce.shuffle.parallelcopies"|"30"|ENSURE
"mapreduce.reduce.speculative"|"false"|ENSURE
"mapreduce.shuffle.port"|"13562"|ENSURE
"mapreduce.task.io.sort.factor"|"100"|ENSURE
"mapreduce.task.io.sort.mb"|"1024"|ENSURE
"mapreduce.task.timeout"|"300000"|ENSURE
"yarn.app.mapreduce.am.admin-command-opts"|"-Dhdp.version=${hdp.version}"|ENSURE
"yarn.app.mapreduce.am.command-opts"|"-Xmx2458m -Dhdp.version=${hdp.version}"|ENSURE
"yarn.app.mapreduce.am.log.level"|"INFO"|ENSURE
"yarn.app.mapreduce.am.resource.mb"|"3072"|ENSURE
"yarn.app.mapreduce.am.staging-dir"|"/user"

[oozie-env]
"content"|"\n#!/bin/bash\n\nif [ -d \"/usr/lib/bigtop-tomcat\" ]; then\n export OOZIE_CONFIG=${OOZIE_CONFIG:-/etc/oozie/conf}\n export CATALINA_BASE=${CATALINA_BASE:-{{oozie_server_dir}}}\n export CATALINA_TMPDIR=${CATALINA_TMPDIR:-/var/tmp/oozie}\n export OOZIE_CATALINA_HOME=/usr/lib/bigtop-tomcat\nfi\n\n#Set JAVA HOME\nexport JAVA_HOME={{java_home}}\n\nexport JRE_HOME=${JAVA_HOME}\n\n# Set Oozie specific environment variables here.\n\n# Settings for the Embedded Tomcat that runs Oozie\n# Java System properties for Oozie should be specified in this variable\n#\n# export CATALINA_OPTS=\n\n# Oozie configuration file to load from Oozie configuration directory\n#\n# export OOZIE_CONFIG_FILE=oozie-site.xml\n\n# Oozie logs directory\n#\nexport OOZIE_LOG={{oozie_log_dir}}\n\n# Oozie pid directory\n#\nexport CATALINA_PID={{pid_file}}\n\n#Location of the data for oozie\nexport OOZIE_DATA={{oozie_data_dir}}\n\n# Oozie Log4J configuration file to load from Oozie configuration directory\n#\n# export OOZIE_LOG4J_FILE=oozie-log4j.properties\n\n# Reload interval of the Log4J configuration file, in seconds\n#\n# export OOZIE_LOG4J_RELOAD=10\n\n# The port Oozie server runs\n#\nexport OOZIE_HTTP_PORT={{oozie_server_port}}\n\n# The admin port Oozie server runs\n#\nexport OOZIE_ADMIN_PORT={{oozie_server_admin_port}}\n\n# The host name Oozie server runs on\n#\n# export OOZIE_HTTP_HOSTNAME=`hostname -f`\n\n# The base URL for callback URLs to Oozie\n#\n# export OOZIE_BASE_URL=\"http://${OOZIE_HTTP_HOSTNAME}:${OOZIE_HTTP_PORT}/oozie\"\nexport JAVA_LIBRARY_PATH={{hadoop_lib_home}}/native/Linux-amd64-64\n\n# At least 1 minute of retry time to account for server downtime during\n# upgrade/downgrade\nexport OOZIE_CLIENT_OPTS=\"${OOZIE_CLIENT_OPTS} -Doozie.connection.retry.count=5 \"\n\n# This is needed so that Oozie does not run into OOM or GC Overhead limit\n# exceeded exceptions. If the oozie server is handling large number of\n# workflows/coordinator jobs, the memory settings may need to be revised\nexport CATALINA_OPTS=\"${CATALINA_OPTS} -Xmx2048m -XX:MaxPermSize=256m \""|ENSURE
"oozie_admin_port"|"11001"|ENSURE
"oozie_ambari_database"|"MySQL"|ENSURE
"oozie_data_dir"|"/hadoop/oozie/data"|ENSURE
"oozie_heapsize"|"2048m"|ENSURE
"oozie_hostname"|""|ENSURE
"oozie_log_dir"|"/hadoop/logs/oozie"|ENSURE
"oozie_permsize"|"256m"|ENSURE
"oozie_pid_dir"|"/var/run/oozie"|ENSURE
"oozie_user"|"oozie"

[oozie-site]
"oozie.authentication.kerberos.name.rules"|"\n RULE:[2:$1@$0]([jt]t@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-MAPREDUSER/\n RULE:[2:$1@$0]([nd]n@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HDFSUSER/\n RULE:[2:$1@$0](hm@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/\n RULE:[2:$1@$0](rs@.*TODO-KERBEROS-DOMAIN)s/.*/TODO-HBASE-USER/\n DEFAULT"|ENSURE
"oozie.authentication.simple.anonymous.allowed"|"true"|ENSURE
"oozie.authentication.type"|"simple"|ENSURE
"oozie.base.url"|"http://localhost:11000/oozie"|ENSURE_ALERT
"oozie.credentials.credentialclasses"|"hcat=org.apache.oozie.action.hadoop.HCatCredentials"|ENSURE
#"oozie.db.schema.name"|"oozie"|ENSURE
"oozie.service.ActionService.executor.ext.classes"|"\n org.apache.oozie.action.email.EmailActionExecutor,\n org.apache.oozie.action.hadoop.HiveActionExecutor,\n org.apache.oozie.action.hadoop.ShellActionExecutor,\n org.apache.oozie.action.hadoop.SqoopActionExecutor,\n org.apache.oozie.action.hadoop.DistcpActionExecutor"|ENSURE
"oozie.service.AuthorizationService.security.enabled"|"true"|ENSURE
"oozie.service.CallableQueueService.callable.concurrency"|"3"|ENSURE
"oozie.service.CallableQueueService.queue.size"|"1000"|ENSURE
"oozie.service.CallableQueueService.threads"|"10"|ENSURE
"oozie.service.HadoopAccessorService.hadoop.configurations"|"*=/etc/hadoop/conf"|ENSURE
"oozie.service.HadoopAccessorService.kerberos.enabled"|"false"|ENSURE
"oozie.service.JPAService.create.db.schema"|"false"|ENSURE
"oozie.service.ProxyUserService.proxyuser.falcon.groups"|"*"|ENSURE
"oozie.service.ProxyUserService.proxyuser.falcon.hosts"|"*"|ENSURE
"oozie.service.ProxyUserService.proxyuser.hue.groups"|"*"|ENSURE
"oozie.service.ProxyUserService.proxyuser.hue.hosts"|"*"|ENSURE
"oozie.service.PurgeService.older.than"|"30"|ENSURE
"oozie.service.PurgeService.purge.interval"|"3600"|ENSURE
"oozie.service.SchemaService.wf.ext.schemas"|"shell-action-0.1.xsd,shell-action-0.2.xsd,shell-action-0.3.xsd,email-action-0.1.xsd,email-action-0.2.xsd,hive-action-0.2.xsd,hive-action-0.3.xsd,hive-action-0.4.xsd,hive-action-0.5.xsd,sqoop-action-0.2.xsd,sqoop-action-0.3.xsd,sqoop-action-0.4.xsd,ssh-action-0.1.xsd,ssh-action-0.2.xsd,distcp-action-0.1.xsd,distcp-action-0.2.xsd,oozie-sla-0.1.xsd,oozie-sla-0.2.xsd"|ENSURE
"oozie.service.URIHandlerService.uri.handlers"|"org.apache.oozie.dependency.FSURIHandler,org.apache.oozie.dependency.HCatURIHandler"|ENSURE
"oozie.service.WorkflowAppService.system.libpath"|"/user/${user.name}/share/lib"|ENSURE
"oozie.service.coord.check.maximum.frequency"|"false"|ENSURE
"oozie.service.coord.normal.default.timeout"|"120"|ENSURE
"oozie.service.coord.push.check.requeue.interval"|"30000"|ENSURE
"oozie.services"|"\n org.apache.oozie.service.SchedulerService,\n org.apache.oozie.service.InstrumentationService,\n org.apache.oozie.service.MemoryLocksService,\n org.apache.oozie.service.UUIDService,\n org.apache.oozie.service.ELService,\n org.apache.oozie.service.AuthorizationService,\n org.apache.oozie.service.UserGroupInformationService,\n org.apache.oozie.service.HadoopAccessorService,\n org.apache.oozie.service.JobsConcurrencyService,\n org.apache.oozie.service.URIHandlerService,\n org.apache.oozie.service.DagXLogInfoService,\n org.apache.oozie.service.SchemaService,\n org.apache.oozie.service.LiteWorkflowAppService,\n org.apache.oozie.service.JPAService,\n org.apache.oozie.service.StoreService,\n org.apache.oozie.service.CoordinatorStoreService,\n org.apache.oozie.service.SLAStoreService,\n org.apache.oozie.service.DBLiteWorkflowStoreService,\n org.apache.oozie.service.CallbackService,\n org.apache.oozie.service.ShareLibService,\n org.apache.oozie.service.CallableQueueService,\n org.apache.oozie.service.ActionService,\n org.apache.oozie.service.ActionCheckerService,\n org.apache.oozie.service.RecoveryService,\n org.apache.oozie.service.PurgeService,\n org.apache.oozie.service.CoordinatorEngineService,\n org.apache.oozie.service.BundleEngineService,\n org.apache.oozie.service.DagEngineService,\n org.apache.oozie.service.CoordMaterializeTriggerService,\n org.apache.oozie.service.StatusTransitService,\n org.apache.oozie.service.PauseTransitService,\n org.apache.oozie.service.GroupsService,\n org.apache.oozie.service.ProxyUserService,\n org.apache.oozie.service.XLogStreamingService,\n org.apache.oozie.service.JvmPauseMonitorService"|ENSURE
"oozie.services.ext"|"org.apache.oozie.service.JMSAccessorService,org.apache.oozie.service.PartitionDependencyManagerService,org.apache.oozie.service.HCatAccessorService"|ENSURE
"oozie.system.id"|"oozie-${user.name}"|ENSURE
"oozie.systemmode"|"NORMAL"|ENSURE
"use.system.libpath.for.mapreduce.and.pig.jobs"|"false"

[tez-site]
"tez.am.am-rm.heartbeat.interval-ms.max"|"250"|ENSURE
"tez.am.container.idle.release-timeout-max.millis"|"20000"|ENSURE
"tez.am.container.idle.release-timeout-min.millis"|"10000"|ENSURE
"tez.am.container.reuse.enabled"|"true"|ENSURE
"tez.am.container.reuse.locality.delay-allocation-millis"|"250"|ENSURE
"tez.am.container.reuse.non-local-fallback.enabled"|"false"|ENSURE
"tez.am.container.reuse.rack-fallback.enabled"|"true"|ENSURE
"tez.am.launch.cluster-default.cmd-opts"|"-server -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}"|ENSURE
"tez.am.launch.cmd-opts"|"-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC"|ENSURE
"tez.am.launch.env"|"LD_LIBRARY_PATH=/usr/hdp/${hdp.version}/hadoop/lib/native:/usr/hdp/${hdp.version}/hadoop/lib/native/Linux-amd64-64"|ENSURE
"tez.am.log.level"|"INFO"|ENSURE
"tez.am.max.app.attempts"|"2"|ENSURE
"tez.am.maxtaskfailures.per.node"|"10"|ENSURE
"tez.am.resource.memory.mb"|"3072"|ENSURE
"tez.cluster.additional.classpath.prefix"|"/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure"|ENSURE
"tez.counters.max"|"2000"|ENSURE
"tez.counters.max.groups"|"1000"|ENSURE
"tez.generate.debug.artifacts"|"false"|ENSURE
"tez.grouping.max-size"|"1073741824"|ENSURE
"tez.grouping.min-size"|"16777216"|ENSURE
"tez.grouping.split-waves"|"1.7"|ENSURE
"tez.history.logging.service.class"|"org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService"|ENSURE
"tez.lib.uris"|"/hdp/apps/${hdp.version}/tez/tez.tar.gz"|ENSURE
"tez.runtime.compress"|"true"|ENSURE
"tez.runtime.compress.codec"|"org.apache.hadoop.io.compress.SnappyCodec"|ENSURE
"tez.runtime.io.sort.mb"|"1228"|ENSURE
"tez.runtime.unordered.output.buffer.size-mb"|"230"|ENSURE
"tez.session.am.dag.submit.timeout.secs"|"300"|ENSURE
"tez.session.client.timeout.secs"|"-1"|ENSURE
"tez.shuffle-vertex-manager.max-src-fraction"|"0.4"|ENSURE
"tez.shuffle-vertex-manager.min-src-fraction"|"0.2"|ENSURE
"tez.staging-dir"|"/tmp/${user.name}/staging"|ENSURE
"tez.task.am.heartbeat.counter.interval-ms.max"|"4000"|ENSURE
"tez.task.get-task.sleep.interval-ms.max"|"200"|ENSURE
"tez.task.launch.cluster-default.cmd-opts"|"-server -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}"|ENSURE
"tez.task.launch.cmd-opts"|"-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC"|ENSURE
"tez.task.launch.env"|"LD_LIBRARY_PATH=/usr/hdp/${hdp.version}/hadoop/lib/native:/usr/hdp/${hdp.version}/hadoop/lib/native/Linux-amd64-64"|ENSURE
"tez.task.max-events-per-heartbeat"|"500"|ENSURE
"tez.task.resource.memory.mb"|"3072"

[webhcat-site]
"templeton.exec.timeout"|"60000"|ENSURE
"templeton.hadoop"|"/usr/hdp/current/hadoop-client/bin/hadoop"|ENSURE
"templeton.hadoop.conf.dir"|"/etc/hadoop/conf"|ENSURE
"templeton.hcat"|"/usr/hdp/current/hive-client/bin/hcat"|ENSURE
"templeton.hcat.home"|"hive.tar.gz/hive/hcatalog"|ENSURE
"templeton.hive.archive"|"hdfs:///hdp/apps/${hdp.version}/hive/hive.tar.gz"|ENSURE
"templeton.hive.home"|"hive.tar.gz/hive"|ENSURE
"templeton.hive.path"|"hive.tar.gz/hive/bin/hive"|ENSURE
"templeton.hive.properties"|"hive.metastore.local=false, hive.metastore.uris=thrift://ip-172-31-5-57.ec2.internal:9933, hive.metastore.sasl.enabled=false, hive.metastore.execute.setugi=true"|ENSURE
"templeton.jar"|"/usr/hdp/current/hive-webhcat/share/webhcat/svr/lib/hive-webhcat-*.jar"|ENSURE
"templeton.libjars"|"/usr/hdp/current/zookeeper-client/zookeeper.jar"|ENSURE
"templeton.override.enabled"|"false"|ENSURE
"templeton.pig.archive"|"hdfs:///hdp/apps/${hdp.version}/pig/pig.tar.gz"|ENSURE
"templeton.pig.path"|"pig.tar.gz/pig/bin/pig"|ENSURE
"templeton.port"|"50111"|ENSURE
"templeton.sqoop.archive"|"hdfs:///hdp/apps/${hdp.version}/sqoop/sqoop.tar.gz"|ENSURE
"templeton.sqoop.home"|"sqoop.tar.gz/sqoop"|ENSURE
"templeton.sqoop.path"|"sqoop.tar.gz/sqoop/bin/sqoop"|ENSURE
"templeton.storage.class"|"org.apache.hive.hcatalog.templeton.tool.ZooKeeperStorage"|ENSURE
"templeton.streaming.jar"|"hdfs:///hdp/apps/${hdp.version}/mapreduce/hadoop-streaming.jar"|ENSURE
"templeton.zookeeper.hosts"|"localhost1:2181,localhost2:2181,localhost3:2181"|ENSURE_ALERT
"webhcat.proxyuser.hue.groups"|"*"|ENSURE
"webhcat.proxyuser.hue.hosts"|"*"

[yarn-env]
"apptimelineserver_heapsize"|"1024"|ENSURE
"content"|"\nexport HADOOP_YARN_HOME={{hadoop_yarn_home}}\nexport YARN_LOG_DIR={{yarn_log_dir_prefix}}/$USER\nexport YARN_PID_DIR={{yarn_pid_dir_prefix}}/$USER\nexport HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\nexport JAVA_HOME={{java64_home}}\n\n# User for YARN daemons\nexport HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn}\n\n# resolve links - $0 may be a softlink\nexport YARN_CONF_DIR=\"${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}\"\n\n# some Java parameters\n# export JAVA_HOME=/home/y/libexec/jdk1.6.0/\nif [ \"$JAVA_HOME\" != \"\" ]; then\n #echo \"run java in $JAVA_HOME\"\n JAVA_HOME=$JAVA_HOME\nfi\n\nif [ \"$JAVA_HOME\" = \"\" ]; then\n echo \"Error: JAVA_HOME is not set.\"\n exit 1\nfi\n\nJAVA=$JAVA_HOME/bin/java\nJAVA_HEAP_MAX=-Xmx1000m\n\n# For setting YARN specific HEAP sizes please use this\n# Parameter and set appropriately\nYARN_HEAPSIZE={{yarn_heapsize}}\n\n# check envvars which might override default args\nif [ \"$YARN_HEAPSIZE\" != \"\" ]; then\n JAVA_HEAP_MAX=\"-Xmx\"\"$YARN_HEAPSIZE\"\"m\"\nfi\n\n# Resource Manager specific parameters\n\n# Specify the max Heapsize for the ResourceManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1000.\n# This value will be overridden by an Xmx setting specified in either YARN_OPTS\n# and/or YARN_RESOURCEMANAGER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_RESOURCEMANAGER_HEAPSIZE={{resourcemanager_heapsize}}\n\n# Specify the JVM options to be used when starting the ResourceManager.\n# These options will be appended to the options specified as YARN_OPTS\n# and therefore may override any similar flags set in YARN_OPTS\n#export YARN_RESOURCEMANAGER_OPTS=\n\n# Node Manager specific parameters\n\n# Specify the max Heapsize for the NodeManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1000.\n# This value will be overridden by an Xmx setting specified in either YARN_OPTS\n# and/or YARN_NODEMANAGER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_NODEMANAGER_HEAPSIZE={{nodemanager_heapsize}}\n\n# Specify the max Heapsize for the HistoryManager using a numerical value\n# in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n# the value to 1024.\n# This value will be overridden by an Xmx setting specified in either YARN_OPTS\n# and/or YARN_HISTORYSERVER_OPTS.\n# If not specified, the default value will be picked from either YARN_HEAPMAX\n# or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\nexport YARN_HISTORYSERVER_HEAPSIZE={{apptimelineserver_heapsize}}\n\n# Specify the JVM options to be used when starting the NodeManager.\n# These options will be appended to the options specified as YARN_OPTS\n# and therefore may override any similar flags set in YARN_OPTS\n#export YARN_NODEMANAGER_OPTS=\n\n# so that filenames w/ spaces are handled correctly in loops below\nIFS=\n\n\n# default log directory and file\nif [ \"$YARN_LOG_DIR\" = \"\" ]; then\n YARN_LOG_DIR=\"$HADOOP_YARN_HOME/logs\"\nfi\nif [ \"$YARN_LOGFILE\" = \"\" ]; then\n YARN_LOGFILE='yarn.log'\nfi\n\n# default policy file for service-level authorization\nif [ \"$YARN_POLICYFILE\" = \"\" ]; then\n YARN_POLICYFILE=\"hadoop-policy.xml\"\nfi\n\n# restore ordinary behaviour\nunset IFS\n\n\nYARN_OPTS=\"$YARN_OPTS -Dhadoop.log.dir=$YARN_LOG_DIR\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.log.dir=$YARN_LOG_DIR\"\nYARN_OPTS=\"$YARN_OPTS -Dhadoop.log.file=$YARN_LOGFILE\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.log.file=$YARN_LOGFILE\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.home.dir=$YARN_COMMON_HOME\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.id.str=$YARN_IDENT_STRING\"\nYARN_OPTS=\"$YARN_OPTS -Dhadoop.root.logger=${YARN_ROOT_LOGGER:-INFO,console}\"\nYARN_OPTS=\"$YARN_OPTS -Dyarn.root.logger=${YARN_ROOT_LOGGER:-INFO,console}\"\nif [ \"x$JAVA_LIBRARY_PATH\" != \"x\" ]; then\n YARN_OPTS=\"$YARN_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH\"\nfi\nYARN_OPTS=\"$YARN_OPTS -Dyarn.policy.file=$YARN_POLICYFILE\""|ENSURE
"min_user_id"|"1000"|ENSURE
"nodemanager_heapsize"|"1024"|ENSURE
"resourcemanager_heapsize"|"1024"|ENSURE
"yarn_heapsize"|"1024"|ENSURE
"yarn_log_dir_prefix"|"/var/log/hadoop-yarn"|ENSURE
"yarn_pid_dir_prefix"|"/var/run/hadoop-yarn"|ENSURE
"yarn_user"|"yarn"

[yarn-site]
"hadoop.registry.rm.enabled"|"false"|ENSURE
"hadoop.registry.zk.quorum"|"localhost1:2181,localhost2:2181,localhost3:2181"|ENSURE
"yarn.acl.enable"|"false"|ENSURE
"yarn.admin.acl"|""|ENSURE
"yarn.application.classpath"|"$HADOOP_CONF_DIR,/usr/hdp/current/hadoop-client/*,/usr/hdp/current/hadoop-client/lib/*,/usr/hdp/current/hadoop-hdfs-client/*,/usr/hdp/current/hadoop-hdfs-client/lib/*,/usr/hdp/current/hadoop-yarn-client/*,/usr/hdp/current/hadoop-yarn-client/lib/*"|ENSURE
"yarn.client.nodemanager-connect.max-wait-ms"|"900000"|ENSURE
"yarn.client.nodemanager-connect.retry-interval-ms"|"10000"|ENSURE
"yarn.log-aggregation-enable"|"true"|ENSURE
"yarn.log-aggregation.retain-seconds"|"2592000"|ENSURE
"yarn.log.server.url"|"http://ip-172-31-5-57.ec2.internal:19888/jobhistory/logs"|ENSURE
"yarn.node-labels.fs-store.retry-policy-spec"|"2000, 500"|ENSURE
"yarn.node-labels.fs-store.root-dir"|"/system/yarn/node-labels"|ENSURE
"yarn.node-labels.manager-class"|"org.apache.hadoop.yarn.server.resourcemanager.nodelabels.MemoryRMNodeLabelsManager"|ENSURE
"yarn.nodemanager.address"|"0.0.0.0:45454"|ENSURE
"yarn.nodemanager.admin-env"|"MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX"|ENSURE
"yarn.nodemanager.aux-services"|"mapreduce_shuffle"|ENSURE
"yarn.nodemanager.aux-services.mapreduce_shuffle.class"|"org.apache.hadoop.mapred.ShuffleHandler"|ENSURE
"yarn.nodemanager.bind-host"|"0.0.0.0"|ENSURE
"yarn.nodemanager.container-executor.class"|"org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"|ENSURE
"yarn.nodemanager.container-monitor.interval-ms"|"3000"|ENSURE
"yarn.nodemanager.delete.debug-delay-sec"|"0"|ENSURE
"yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage"|"90"|ENSURE
"yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb"|"1000"|ENSURE
"yarn.nodemanager.disk-health-checker.min-healthy-disks"|"0.25"|ENSURE
"yarn.nodemanager.health-checker.interval-ms"|"135000"|ENSURE
"yarn.nodemanager.health-checker.script.timeout-ms"|"60000"|ENSURE
"yarn.nodemanager.linux-container-executor.cgroups.hierarchy"|"hadoop-yarn"|ENSURE
"yarn.nodemanager.linux-container-executor.cgroups.mount"|"false"|ENSURE
"yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage"|"false"|ENSURE
"yarn.nodemanager.linux-container-executor.group"|"hadoop"|ENSURE
"yarn.nodemanager.linux-container-executor.resources-handler.class"|"org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler"|ENSURE
"yarn.nodemanager.local-dirs"|"/tmp/data/hadoop/yarn/local"|ENSURE
"yarn.nodemanager.log-aggregation.compression-type"|"gz"|ENSURE
"yarn.nodemanager.log-aggregation.debug-enabled"|"false"|ENSURE
"yarn.nodemanager.log-aggregation.num-log-files-per-app"|"30"|ENSURE
"yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds"|"-1"|ENSURE
"yarn.nodemanager.log-dirs"|"/tmp/data/hadoop/yarn/log"|ENSURE
"yarn.nodemanager.log.retain-second"|"604800"|ENSURE
"yarn.nodemanager.recovery.dir"|"/var/log/hadoop-yarn/nodemanager/recovery-state"|ENSURE
"yarn.nodemanager.recovery.enabled"|"false"|ENSURE
"yarn.nodemanager.remote-app-log-dir"|"/app-logs"|ENSURE
"yarn.nodemanager.remote-app-log-dir-suffix"|"logs"|ENSURE
"yarn.nodemanager.resource.cpu-vcores"|"4"|ENSURE
"yarn.nodemanager.resource.memory-mb"|"12288"|ENSURE
"yarn.nodemanager.resource.percentage-physical-cpu-limit"|"100"|ENSURE
"yarn.nodemanager.vmem-check-enabled"|"false"|ENSURE
"yarn.nodemanager.vmem-pmem-ratio"|"2.1"|ENSURE
"yarn.resourcemanager.address"|"localhost:8050"|ENSURE_ALERT
"yarn.resourcemanager.admin.address"|"localhost:8141"|ENSURE_ALERT
"yarn.resourcemanager.am.max-attempts"|"2"|ENSURE
"yarn.resourcemanager.bind-host"|"0.0.0.0"|ENSURE
"yarn.resourcemanager.connect.max-wait.ms"|"900000"|ENSURE
"yarn.resourcemanager.connect.retry-interval.ms"|"30000"|ENSURE
"yarn.resourcemanager.fs.state-store.retry-policy-spec"|"2000, 500"|ENSURE
"yarn.resourcemanager.fs.state-store.uri"|" "|ENSURE
"yarn.resourcemanager.ha.enabled"|"false"|ENSURE
"yarn.resourcemanager.hostname"|"ip-172-31-14-9.ec2.internal"|ENSURE
"yarn.resourcemanager.nodes.exclude-path"|"/etc/hadoop/conf/yarn.exclude"|ENSURE
"yarn.resourcemanager.recovery.enabled"|"false"|ENSURE
"yarn.resourcemanager.resource-tracker.address"|"localhost:8025"|ENSURE_ALERT
"yarn.resourcemanager.scheduler.address"|"localhost:8030"|ENSURE_ALERT
"yarn.resourcemanager.scheduler.class"|"org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"|ENSURE
"yarn.resourcemanager.state-store.max-completed-applications"|"${yarn.resourcemanager.max-completed-applications}"|ENSURE
"yarn.resourcemanager.store.class"|"org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"|ENSURE
"yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size"|"10"|ENSURE
"yarn.resourcemanager.system-metrics-publisher.enabled"|"true"|ENSURE
"yarn.resourcemanager.webapp.address"|"localhost:8088"|ENSURE_ALERT
"yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled"|"false"|ENSURE
"yarn.resourcemanager.work-preserving-recovery.enabled"|"false"|ENSURE
"yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms"|"10000"|ENSURE
"yarn.resourcemanager.zk-acl"|"world:anyone:rwcda"|ENSURE
"yarn.resourcemanager.zk-address"|"localhost:2181"|ENSURE_ALERT
"yarn.resourcemanager.zk-num-retries"|"1000"|ENSURE
"yarn.resourcemanager.zk-retry-interval-ms"|"1000"|ENSURE
"yarn.resourcemanager.zk-state-store.parent-path"|"/rmstore"|ENSURE
"yarn.resourcemanager.zk-timeout-ms"|"10000"|ENSURE
"yarn.scheduler.maximum-allocation-mb"|"12288"|ENSURE
"yarn.scheduler.minimum-allocation-mb"|"3072"|ENSURE
"yarn.timeline-service.address"|"localhost:10200"|ENSURE_ALERT
"yarn.timeline-service.bind-host"|"0.0.0.0"|ENSURE
"yarn.timeline-service.client.max-retries"|"30"|ENSURE
"yarn.timeline-service.client.retry-interval-ms"|"1000"|ENSURE
"yarn.timeline-service.enabled"|"true"|ENSURE
"yarn.timeline-service.generic-application-history.store-class"|"org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore"|ENSURE
"yarn.timeline-service.http-authentication.simple.anonymous.allowed"|"true"|ENSURE
"yarn.timeline-service.http-authentication.type"|"simple"|ENSURE
"yarn.timeline-service.leveldb-timeline-store.path"|"/hadoop/yarn/timeline"|ENSURE
"yarn.timeline-service.leveldb-timeline-store.read-cache-size"|"104857600"|ENSURE
"yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size"|"10000"|ENSURE
"yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size"|"10000"|ENSURE
"yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms"|"300000"|ENSURE
"yarn.timeline-service.store-class"|"org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"|ENSURE
"yarn.timeline-service.ttl-enable"|"true"|ENSURE
"yarn.timeline-service.ttl-ms"|"2678400000"|ENSURE
"yarn.timeline-service.webapp.address"|"localhost:8188"|ENSURE_ALERT
"yarn.timeline-service.webapp.https.address"|"localhost:8190"|ENSURE_ALERT

